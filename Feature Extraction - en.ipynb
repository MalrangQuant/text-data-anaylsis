{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction - English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이선우 (20223888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) NLTK & gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #토큰화\n",
    "import string #Feature Extraction 할때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (text): #from nltk import word_tokenize 일케 해두 되긴함\n",
    "    text = text.lower()\n",
    "    stemmer = nltk.stem.SnowballStemmer('english') #stemmer는 기계적으로 어미자르기 등등 (nltk stem 안에 뭐 있는지 보면 이것저것 나옴)\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        yield stemmer.stem(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(list(tokenize(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [list(tokenize(doc)) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'eleph', 'sneez', 'at', 'the', 'sight', 'of', 'potato']\n",
      "['bat', 'can', 'see', 'via', 'echoloc', 'see', 'the', 'bat', 'sight', 'sneez']\n",
      "['wonder', 'she', 'open', 'the', 'door', 'to', 'the', 'studio']\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = gensim.corpora.Dictionary(tokenized_corpus) #토큰화 시킨 텍스트를 읽으면서 gensim이 세서 각 vocabulary에 숫자를 주어서 차원으로 보내줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'at')\n",
      "(1, 'eleph')\n",
      "(2, 'of')\n",
      "(3, 'potato')\n",
      "(4, 'sight')\n",
      "(5, 'sneez')\n",
      "(6, 'the')\n",
      "(7, 'bat')\n",
      "(8, 'can')\n",
      "(9, 'echoloc')\n",
      "(10, 'see')\n",
      "(11, 'via')\n",
      "(12, 'door')\n",
      "(13, 'open')\n",
      "(14, 'she')\n",
      "(15, 'studio')\n",
      "(16, 'to')\n",
      "(17, 'wonder')\n"
     ]
    }
   ],
   "source": [
    "for x in lexicon.items():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)]\n",
      "[(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc) #벡터로 바꿔줘 \n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 거가 Feature Extraction임 (벡터로 바꿔주는것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse vector <=> dense vector\n",
    "\n",
    "벡터를 표현하는 두가지 방법이 있다. \n",
    "\n",
    "sparse의 경우 0을 줠래 찍는 것보다 효율적이다. \n",
    "\n",
    "필요할때는 dense로 전환함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer() #요런걸 인스턴스화 한다는 것 (객체로 사용) (내가 쓰겟다구한것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 2 0 0 0]\n",
      " [0 1 1 1 0 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(results.A) #단어를 찍어서 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 확인하는 방법은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 16,\n",
       " 'elephant': 6,\n",
       " 'sneezed': 14,\n",
       " 'at': 0,\n",
       " 'sight': 12,\n",
       " 'of': 7,\n",
       " 'potatoes': 9,\n",
       " 'bats': 2,\n",
       " 'can': 3,\n",
       " 'see': 10,\n",
       " 'via': 18,\n",
       " 'echolocation': 5,\n",
       " 'bat': 1,\n",
       " 'sneeze': 13,\n",
       " 'wondering': 19,\n",
       " 'she': 11,\n",
       " 'opened': 8,\n",
       " 'door': 4,\n",
       " 'to': 17,\n",
       " 'studio': 15}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=results.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse = { v: k for k, v in vectorizer.vocabulary_.items() } # (단어 > 숫자) 에서 (숫자 > 단어) 로 바꿔준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: 'the',\n",
       " 6: 'elephant',\n",
       " 14: 'sneezed',\n",
       " 0: 'at',\n",
       " 12: 'sight',\n",
       " 7: 'of',\n",
       " 9: 'potatoes',\n",
       " 2: 'bats',\n",
       " 3: 'can',\n",
       " 10: 'see',\n",
       " 18: 'via',\n",
       " 5: 'echolocation',\n",
       " 1: 'bat',\n",
       " 13: 'sneeze',\n",
       " 19: 'wondering',\n",
       " 11: 'she',\n",
       " 8: 'opened',\n",
       " 4: 'door',\n",
       " 17: 'to',\n",
       " 15: 'studio'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>at</th>\n",
       "      <th>bat</th>\n",
       "      <th>bats</th>\n",
       "      <th>can</th>\n",
       "      <th>door</th>\n",
       "      <th>echolocation</th>\n",
       "      <th>elephant</th>\n",
       "      <th>of</th>\n",
       "      <th>opened</th>\n",
       "      <th>potatoes</th>\n",
       "      <th>see</th>\n",
       "      <th>she</th>\n",
       "      <th>sight</th>\n",
       "      <th>sneeze</th>\n",
       "      <th>sneezed</th>\n",
       "      <th>studio</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>via</th>\n",
       "      <th>wondering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   at  bat  bats  can  door  echolocation  elephant  of  opened  potatoes  \\\n",
       "0   1    0     0    0     0             0         1   1       0         1   \n",
       "1   0    1     1    1     0             1         0   0       0         0   \n",
       "2   0    0     0    0     1             0         0   0       1         0   \n",
       "\n",
       "   see  she  sight  sneeze  sneezed  studio  the  to  via  wondering  \n",
       "0    0    0      1       0        1       0    2   0    0          0  \n",
       "1    2    0      1       1        0       0    1   0    1          0  \n",
       "2    0    1      0       0        0       1    2   1    0          1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare nltk & sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn 어근 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'elephant', 'sneezed', 'at', 'sight', 'of', 'potatoes', 'bats', 'can', 'see', 'via', 'echolocation', 'bat', 'sneeze', 'wondering', 'she', 'opened', 'door', 'to', 'studio'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk 어근 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at',\n",
       " 'eleph',\n",
       " 'of',\n",
       " 'potato',\n",
       " 'sight',\n",
       " 'sneez',\n",
       " 'the',\n",
       " 'bat',\n",
       " 'can',\n",
       " 'echoloc',\n",
       " 'see',\n",
       " 'via',\n",
       " 'door',\n",
       " 'open',\n",
       " 'she',\n",
       " 'studio',\n",
       " 'to',\n",
       " 'wonder']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lexicon.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bats',\n",
       " 'echolocation',\n",
       " 'elephant',\n",
       " 'opened',\n",
       " 'potatoes',\n",
       " 'sneeze',\n",
       " 'sneezed',\n",
       " 'wondering'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sklearn에선 있으나 nltk엔 없는것'''\n",
    "set(vectorizer.vocabulary_.keys()) - set(list(lexicon.values())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'echoloc', 'eleph', 'open', 'potato', 'sneez', 'wonder'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nlkt에선 있으나 sklearn엔 없는것'''\n",
    "set(list(lexicon.values())) - set(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 비교해보면 stemmer가 있고 없고의 차이를 알 수 있다.\n",
    "\n",
    "sklearn에서는 stemmer가 없어서, sneeze와 sneezed가 다른 단어로 나옴 (복수, 단수 등의 차이를 알 수 없다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat이나 dog가 언급되는 횟수는 중요하지 않을수도 있다아, 그래서 나온거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gensim\n",
    "\n",
    "튜플 (dim, freq) 에서 강제로 freq를 1로 할당 (즉 있냐 없냐만 보겠다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "[(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
      "[(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = [(x[0], 1) for x in vec]\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Scikit-Learn\n",
    "\n",
    "Binarizer를 활용 (0과 1로 바꿔주겠다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 1]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = onehot.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf (Term frequency to Inverse document frequency)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    tf (t, d) &= 1 + \\log \\, f_{t,d}  \\\\\n",
    "    idf (t,D) &= \\log 1 + \\frac{N}{n_t} \\\\\n",
    "    tf-idf (t, d, D) &= tf (t,d) \\cdot idf (t,D)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(dictionary=lexicon, normalize=True) #norm은 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('at', 0.4837965208957426), ('eleph', 0.4837965208957426), ('of', 0.4837965208957426), ('potato', 0.4837965208957426), ('sight', 0.17855490118826325), ('sneez', 0.17855490118826325)]\n",
      "[('sight', 0.10992597952954358), ('sneez', 0.10992597952954358), ('bat', 0.5956913654963344), ('can', 0.2978456827481672), ('echoloc', 0.2978456827481672), ('see', 0.5956913654963344), ('via', 0.2978456827481672)]\n",
      "[('door', 0.408248290463863), ('open', 0.408248290463863), ('she', 0.408248290463863), ('studio', 0.408248290463863), ('to', 0.408248290463863), ('wonder', 0.408248290463863)]\n"
     ]
    }
   ],
   "source": [
    "for doc in tokenized_corpus:\n",
    "    vec = lexicon.doc2bow(doc)\n",
    "    vec = tfidf[vec] #위치 찍힌거 좌표를 보여줌\n",
    "    vec = [(tfidf.id2word[x[0]], x[1]) for x in vec] #일케 위치를 보여주는뎅 The 가 사라짐 (모든 문서에서 나와서 자동으로 사라지게 되었음, 모든 문서에 포함되는 단어는 알아서 빼준다)\n",
    "\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37867627 0.         0.         0.         0.         0.\n",
      "  0.37867627 0.37867627 0.         0.37867627 0.         0.\n",
      "  0.28799306 0.         0.37867627 0.         0.44730461 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.30251368 0.30251368 0.30251368 0.         0.30251368\n",
      "  0.         0.         0.         0.         0.60502736 0.\n",
      "  0.23006945 0.30251368 0.         0.         0.17866945 0.\n",
      "  0.30251368 0.        ]\n",
      " [0.         0.         0.         0.         0.36772387 0.\n",
      "  0.         0.         0.36772387 0.         0.         0.36772387\n",
      "  0.         0.         0.         0.36772387 0.43436728 0.36772387\n",
      "  0.         0.36772387]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d49957daa954e6bec9f177fc046718ee074770fab86eb9520507fb7fbdbef1cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
